"""
ç¯å¢ƒæ£€æŸ¥è„šæœ¬ - é€‚ç”¨äºæœ‰å¡å’Œæ— å¡æ¨¡å¼
è¿è¡Œ: python check_env.py
"""
import sys
import os
import glob

def check_environment():
    """æ£€æŸ¥AutoDLç¯å¢ƒé…ç½®"""
    print("=" * 50)
    print("ğŸ” AutoDLç¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹å’Œæ•°æ®é›†
    print("\nğŸ“ æ£€æŸ¥æœ¬åœ°æ–‡ä»¶:")
    
    # æ£€æŸ¥æ¨¡å‹
    model_cache = "models--Qwen--Qwen2.5-1.5B"
    if os.path.exists(model_cache):
        snapshots = glob.glob(os.path.join(model_cache, "snapshots", "*"))
        if snapshots:
            print(f"   âœ… æœ¬åœ°æ¨¡å‹: {snapshots[0]}")
        else:
            print(f"   âš ï¸  æ¨¡å‹ç¼“å­˜æ–‡ä»¶å¤¹å­˜åœ¨ä½†æ— å¿«ç…§")
    else:
        print(f"   âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä»ç½‘ç»œä¸‹è½½")
    
    # æ£€æŸ¥æ•°æ®é›†
    dataset_cache = "datasets--lansinuote--ChnSentiCorp"
    if os.path.exists(dataset_cache):
        snapshots = glob.glob(os.path.join(dataset_cache, "snapshots", "*"))
        if snapshots:
            print(f"   âœ… æœ¬åœ°æ•°æ®é›†: {snapshots[0]}")
        else:
            print(f"   âš ï¸  æ•°æ®é›†ç¼“å­˜æ–‡ä»¶å¤¹å­˜åœ¨ä½†æ— å¿«ç…§")
    else:
        print(f"   âš ï¸  æœ¬åœ°æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†ä»ç½‘ç»œä¸‹è½½")
    
    # æ£€æŸ¥ HuggingFace é•œåƒè®¾ç½®
    hf_endpoint = os.environ.get('HF_ENDPOINT', 'æœªè®¾ç½®')
    print(f"\nğŸŒ HuggingFaceé•œåƒ: {hf_endpoint}")
    if hf_endpoint == 'æœªè®¾ç½®':
        print("   ğŸ’¡ å»ºè®®è®¾ç½®: export HF_ENDPOINT=https://hf-mirror.com")
    
    print("=" * 50)
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    print(f"\nğŸ“Œ Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥PyTorch
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        
        # æ£€æŸ¥CUDA
        cuda_available = torch.cuda.is_available()
        print(f"{'âœ…' if cuda_available else 'âš ï¸ '} CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                # æ˜¾ç¤ºæ˜¾å­˜ä¿¡æ¯
                mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   æ˜¾å­˜: {mem_total:.1f} GB")
        else:
            print("   âš ï¸  æ— GPUæ£€æµ‹ï¼ˆæ— å¡å¼€æœºæ¨¡å¼ï¼‰")
            print("   ğŸ’¡ è®­ç»ƒå‰éœ€è¦å¼€å¡ï¼")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥å…³é”®ä¾èµ–
    modules = {
        'transformers': 'transformers',
        'datasets': 'datasets', 
        'peft': 'peft',
        'bitsandbytes': 'bitsandbytes',
        'accelerate': 'accelerate',
        'sklearn': 'scikit-learn',
    }
    
    print("\nğŸ“¦ æ£€æŸ¥é¡¹ç›®ä¾èµ–:")
    all_installed = True
    for module, package in modules.items():
        try:
            mod = __import__(module)
            version = getattr(mod, '__version__', 'unknown')
            print(f"   âœ… {package}: {version}")
        except ImportError:
            print(f"   âŒ {package}: æœªå®‰è£…")
            all_installed = False
    
    print("\n" + "=" * 50)
    if all_installed and torch.cuda.is_available():
        print("ğŸ‰ ç¯å¢ƒé…ç½®å®Œç¾ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
        print("   è¿è¡Œ: python train.py")
    elif all_installed:
        print("âœ… ä¾èµ–å·²å®‰è£…ï¼Œä½†éœ€è¦å¼€å¡æ‰èƒ½è®­ç»ƒ")
        print("   ğŸ’¡ åœ¨AutoDLæ§åˆ¶å°å¼€å¡åå³å¯è®­ç»ƒ")
    else:
        print("âš ï¸  è¯·å…ˆå®‰è£…ä¾èµ–:")
        print("   pip install -r requirements_autodl.txt -i https://pypi.tuna.tsinghua.edu.cn/simple")
    print("=" * 50)
    
    return all_installed

if __name__ == "__main__":
    check_environment()
