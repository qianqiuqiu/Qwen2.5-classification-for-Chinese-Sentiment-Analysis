"""
å¾®è°ƒä¸»è„šæœ¬
ä½¿ç”¨ LoRA/QLoRA å¾®è°ƒ Qwen-1.5B è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æ

ä½¿ç”¨æ–¹æ³•:
    python train.py                          # ä½¿ç”¨é»˜è®¤é…ç½®
    python train.py --use_qlora              # ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰
    python train.py --lora_r 16              # è‡ªå®šä¹‰ LoRA ç§©
    python train.py --num_epochs 5           # è‡ªå®šä¹‰è®­ç»ƒè½®æ•°
"""

import os
import argparse
import torch
import numpy as np
from typing import Dict, Any
import wandb
import glob

# è®¾ç½® HuggingFace é•œåƒï¼ˆç”¨äºåœ¨çº¿ä¸‹è½½æ—¶åŠ é€Ÿï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    BitsAndBytesConfig,
)
from peft import get_peft_model, prepare_model_for_kbit_training

# å¯¼å…¥é¡¹ç›®é…ç½®
from configs import (
    get_lora_config,
    LORA_CONFIGS,
    ModelConfig,
    DataConfig,
    TrainingConfig,
    QLoRAConfig,
    get_training_args,
)
from data import load_sentiment_dataset, create_tokenized_dataset, get_data_collator

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


def get_local_model_path(model_name: str) -> str:
    """
    æ£€æµ‹å¹¶è¿”å›æœ¬åœ°æ¨¡å‹è·¯å¾„
    
    Args:
        model_name: HuggingFace æ¨¡å‹åç§°ï¼Œå¦‚ "Qwen/Qwen2.5-1.5B"
    
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–åŸå§‹æ¨¡å‹åç§°
    """
    # å°† HuggingFace æ ¼å¼è½¬æ¢ä¸ºæœ¬åœ°ç¼“å­˜è·¯å¾„æ ¼å¼
    # "Qwen/Qwen2.5-1.5B" -> "models--Qwen--Qwen2.5-1.5B"
    cache_folder = "models--" + model_name.replace("/", "--")
    
    # åœ¨å½“å‰ç›®å½•ä¸‹æŸ¥æ‰¾
    if os.path.exists(cache_folder):
        # æŸ¥æ‰¾ snapshots ç›®å½•ä¸­çš„æ¨¡å‹
        snapshot_pattern = os.path.join(cache_folder, "snapshots", "*")
        snapshots = glob.glob(snapshot_pattern)
        if snapshots:
            model_path = snapshots[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¿«ç…§
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {model_path}")
            return model_path
    
    print(f"ğŸŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä» HuggingFace ä¸‹è½½: {model_name}")
    return model_name


def get_local_dataset_path(dataset_name: str) -> tuple:
    """
    æ£€æµ‹å¹¶è¿”å›æœ¬åœ°æ•°æ®é›†è·¯å¾„
    
    Args:
        dataset_name: HuggingFace æ•°æ®é›†åç§°ï¼Œå¦‚ "lansinuote/ChnSentiCorp"
    
    Returns:
        (æ˜¯å¦ä½¿ç”¨æœ¬åœ°, æ•°æ®é›†è·¯å¾„)
    """
    # æ„å»ºç¼“å­˜æ–‡ä»¶å¤¹åç§°
    cache_folder = "datasets--" + dataset_name.replace("/", "--")
    
    # åœ¨å½“å‰ç›®å½•ä¸‹æŸ¥æ‰¾
    if os.path.exists(cache_folder):
        # æŸ¥æ‰¾ snapshots ç›®å½•
        snapshot_pattern = os.path.join(cache_folder, "snapshots", "*")
        snapshots = glob.glob(snapshot_pattern)
        if snapshots:
            dataset_path = snapshots[0]
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ•°æ®é›†: {dataset_path}")
            return True, dataset_path
    
    print(f"ğŸŒ æœ¬åœ°æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†ä» HuggingFace ä¸‹è½½: {dataset_name}")
    return False, dataset_name


def get_last_checkpoint(output_dir: str) -> str:
    """
    è·å–æœ€æ–°çš„ checkpoint è·¯å¾„
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        æœ€æ–° checkpoint çš„è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
    """
    if not os.path.isdir(output_dir):
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰ checkpoint ç›®å½•
    checkpoints = [
        os.path.join(output_dir, d)
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))
    ]
    
    if not checkpoints:
        return None
    
    # è¿”å›æœ€æ–°çš„ checkpointï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼‰
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    return latest_checkpoint


def compute_metrics(eval_pred) -> Dict[str, float]:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    Args:
        eval_pred: åŒ…å«é¢„æµ‹ç»“æœå’Œæ ‡ç­¾çš„å…ƒç»„
    
    Returns:
        æŒ‡æ ‡å­—å…¸
    """
    predictions, labels = eval_pred
    
    # å¦‚æœæ˜¯ logitsï¼Œå– argmax
    if len(predictions.shape) > 1:
        predictions = np.argmax(predictions, axis=-1)
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary'
    )
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }


def setup_model_and_tokenizer(
    model_config: ModelConfig,
    qlora_config: QLoRAConfig,
) -> tuple:
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_config: æ¨¡å‹é…ç½®
        qlora_config: QLoRA é…ç½®
    
    Returns:
        (model, tokenizer) å…ƒç»„
    """
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_config.model_name_or_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_config.model_name_or_path,
        trust_remote_code=model_config.trust_remote_code,
    )
    
    # ç¡®ä¿æœ‰ pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é…ç½®é‡åŒ–ï¼ˆQLoRAï¼‰
    quantization_config = None
    if qlora_config.use_qlora:
        print("å¯ç”¨ QLoRA 4-bit é‡åŒ–...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=qlora_config.load_in_4bit,
            bnb_4bit_quant_type=qlora_config.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=qlora_config.bnb_4bit_use_double_quant,
        )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        model_config.model_name_or_path,
        num_labels=model_config.num_labels,
        trust_remote_code=model_config.trust_remote_code,
        quantization_config=quantization_config,
        device_map="auto" if qlora_config.use_qlora else None,
        torch_dtype=torch.bfloat16,
    )
    
    # è®¾ç½® pad token id
    model.config.pad_token_id = tokenizer.pad_token_id
    
    # å¦‚æœä½¿ç”¨ QLoRAï¼Œå‡†å¤‡æ¨¡å‹
    if qlora_config.use_qlora:
        model = prepare_model_for_kbit_training(model)
    
    return model, tokenizer


def setup_lora(
    model,
    lora_r: int = 8,
    lora_alpha: int = 32,
    lora_dropout: float = 0.1,
    use_qlora: bool = False,
):
    """
    ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨
    
    Args:
        model: åŸºç¡€æ¨¡å‹
        lora_r: LoRA ç§©
        lora_alpha: LoRA alpha
        lora_dropout: Dropout æ¦‚ç‡
        use_qlora: æ˜¯å¦ä½¿ç”¨ QLoRA
    
    Returns:
        å¸¦æœ‰ LoRA çš„æ¨¡å‹
    """
    
    print(f"é…ç½® LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
    
    lora_config = get_lora_config(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        use_qlora=use_qlora,
    )
    
    # åº”ç”¨ LoRA
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    model.print_trainable_parameters()
    
    return model


def train(args: argparse.Namespace):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    
    # ==================== 1. é…ç½®åˆå§‹åŒ– ====================
    # æ£€æµ‹æœ¬åœ°æ¨¡å‹
    local_model_path = get_local_model_path(args.model_name)
    
    model_config = ModelConfig(
        model_name_or_path=local_model_path,
        num_labels=2,
    )
    
    data_config = DataConfig(
        dataset_name=args.dataset,
        max_length=args.max_length,
    )
    
    training_config = TrainingConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        gradient_checkpointing=args.gradient_checkpointing,
    )
    
    qlora_config = QLoRAConfig(
        use_qlora=args.use_qlora,
    )
    
    # ==================== 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ====================
    model, tokenizer = setup_model_and_tokenizer(model_config, qlora_config)
    
    # ==================== 3. åº”ç”¨ LoRA ====================
    model = setup_lora(
        model=model,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        use_qlora=args.use_qlora,
    )
    
    # ==================== 4. åŠ è½½å’Œå¤„ç†æ•°æ® ====================
    print(f"\næ­£åœ¨åŠ è½½æ•°æ®é›†: {data_config.dataset_name}")
    
    # æ£€æµ‹æœ¬åœ°æ•°æ®é›†
    use_local, dataset_path = get_local_dataset_path("lansinuote/ChnSentiCorp")
    dataset = load_sentiment_dataset(data_config.dataset_name, local_path=dataset_path if use_local else None)
    
    print("æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...")
    tokenized_dataset = create_tokenized_dataset(
        dataset=dataset,
        tokenizer=tokenizer,
        max_length=data_config.max_length,
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(tokenized_dataset['train'])}")
    print(f"éªŒè¯é›†å¤§å°: {len(tokenized_dataset['validation'])}")
    
    # ==================== 5. åˆå§‹åŒ– wandb ====================
    wandb.init(
        project="qwen-sentiment-analysis",
        name=f"lora-r{args.lora_r}-{args.dataset}",
        config={
            "model_name": args.model_name,
            "dataset": args.dataset,
            "lora_r": args.lora_r,
            "lora_alpha": args.lora_alpha,
            "lora_dropout": args.lora_dropout,
            "use_qlora": args.use_qlora,
            "num_epochs": args.num_epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "max_length": args.max_length,
        },
        tags=["LoRA", "QLoRA" if args.use_qlora else "LoRA", "sentiment-analysis"],
    )
    
    # ==================== 6. é…ç½®è®­ç»ƒå‚æ•° ====================
    training_args = TrainingArguments(
        **get_training_args(training_config)
    )
    
    # ==================== 7. åˆå§‹åŒ– Trainer ====================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
        data_collator=get_data_collator(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[
            EarlyStoppingCallback(early_stopping_patience=3),
        ] if args.early_stopping else [],
    )
    
    # ==================== 8. æ£€æµ‹æ–­ç‚¹ ====================
    checkpoint = None
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint == "auto":
            # è‡ªåŠ¨æ£€æµ‹æœ€æ–° checkpoint
            checkpoint = get_last_checkpoint(args.output_dir)
            if checkpoint:
                print(f"\næ£€æµ‹åˆ°æ–­ç‚¹: {checkpoint}")
                print("å°†ä»æ–­ç‚¹æ¢å¤è®­ç»ƒ...\n")
            else:
                print("\næœªæ£€æµ‹åˆ°å¯ç”¨çš„ checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ...\n")
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ checkpoint è·¯å¾„
            checkpoint = args.resume_from_checkpoint
            if os.path.isdir(checkpoint):
                print(f"\nä»æŒ‡å®šæ–­ç‚¹æ¢å¤: {checkpoint}\n")
            else:
                print(f"\nè­¦å‘Š: æŒ‡å®šçš„ checkpoint ä¸å­˜åœ¨: {checkpoint}")
                print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...\n")
                checkpoint = None
    
    # ==================== 9. å¼€å§‹è®­ç»ƒ ====================
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒ...")
    print("=" * 50 + "\n")
    
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    
    # ==================== 10. ä¿å­˜æ¨¡å‹ ====================
    print("\nä¿å­˜æ¨¡å‹...")
    
    # ä¿å­˜ LoRA é€‚é…å™¨
    lora_save_path = os.path.join(args.output_dir, "lora_adapter")
    model.save_pretrained(lora_save_path)
    tokenizer.save_pretrained(lora_save_path)
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    trainer.save_metrics("train", train_result.metrics)
    
    # ==================== 11. æœ€ç»ˆè¯„ä¼° ====================
    print("\nåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    if "test" in tokenized_dataset:
        test_results = trainer.evaluate(tokenized_dataset["test"])
        print(f"\næµ‹è¯•é›†ç»“æœï¼š")
        for key, value in test_results.items():
            print(f"  {key}: {value:.4f}")
        trainer.save_metrics("test", test_results)
        
        # è®°å½•æµ‹è¯•ç»“æœåˆ° wandb
        wandb.log({f"test/{key}": value for key, value in test_results.items()})
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {lora_save_path}")
    
    # ç»“æŸ wandb è¿è¡Œ
    wandb.finish()
    
    return trainer


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ LoRA å¾®è°ƒ Qwen è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æ"
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-1.5B",
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„",
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--dataset",
        type=str,
        default="ChnSentiCorp",
        choices=["ChnSentiCorp", "IMDB_Chinese"],
        help="æ•°æ®é›†åç§°",
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=256,
        help="æœ€å¤§åºåˆ—é•¿åº¦",
    )
    
    # LoRA å‚æ•°
    parser.add_argument(
        "--lora_r",
        type=int,
        default=8,
        help="LoRA ç§©",
    )
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=32,
        help="LoRA alpha",
    )
    parser.add_argument(
        "--lora_dropout",
        type=float,
        default=0.1,
        help="LoRA dropout",
    )
    parser.add_argument(
        "--use_qlora",
        action="store_true",
        help="ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰",
    )
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=3,
        help="è®­ç»ƒè½®æ•°",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=8,
        help="æ‰¹æ¬¡å¤§å°",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-4,
        help="å­¦ä¹ ç‡",
    )
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        default=True,
        help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰",
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        default=True,
        help="å¯ç”¨æ—©åœ",
    )
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default="auto",
        help="ä» checkpoint æ¢å¤è®­ç»ƒã€‚'auto' è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹æœ€æ–° checkpointï¼Œä¹Ÿå¯ä»¥æŒ‡å®šå…·ä½“è·¯å¾„ï¼Œ'none' è¡¨ç¤ºä¸æ¢å¤",
    )
    
    args = parser.parse_args()
    
    # å¤„ç† resume_from_checkpoint å‚æ•°
    if args.resume_from_checkpoint.lower() == "none":
        args.resume_from_checkpoint = None
    
    return args


if __name__ == "__main__":
    args = parse_args()
    
    print("=" * 60)
    print("ä¸­æ–‡æƒ…æ„Ÿåˆ†æ - LoRA å¾®è°ƒ")
    print("=" * 60)
    print(f"\né…ç½®ä¿¡æ¯ï¼š")
    print(f"  æ¨¡å‹: {args.model_name}")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  LoRA r: {args.lora_r}")
    print(f"  QLoRA: {'æ˜¯' if args.use_qlora else 'å¦'}")
    print(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print()
    
    train(args)
