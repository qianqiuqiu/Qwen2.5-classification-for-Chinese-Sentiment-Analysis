"""
æ•°æ®åŠ è½½æ¨¡å—
è´Ÿè´£ä» HuggingFace Hub æˆ–æœ¬åœ°åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†
"""

from datasets import load_dataset, DatasetDict, Dataset
from transformers import DataCollatorWithPadding, PreTrainedTokenizer
from typing import Optional, Dict, Any
import os


def load_sentiment_dataset(
    dataset_name: str = "ChnSentiCorp",
    cache_dir: Optional[str] = None,
    local_path: Optional[str] = None,
) -> DatasetDict:
    """
    åŠ è½½æƒ…æ„Ÿåˆ†ææ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼Œæ”¯æŒ "ChnSentiCorp" æˆ– "IMDB_Chinese"
        cache_dir: ç¼“å­˜ç›®å½•
        local_path: æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼ˆHuggingFace ç¼“å­˜æ ¼å¼ï¼‰
    
    Returns:
        DatasetDict: åŒ…å« train/validation/test çš„æ•°æ®é›†
    
    æ•°æ®é›†æ ¼å¼ï¼š
        - text: è¯„è®ºæ–‡æœ¬
        - label: 0ï¼ˆè´Ÿé¢ï¼‰æˆ– 1ï¼ˆæ­£é¢ï¼‰
    """
    
    if dataset_name == "ChnSentiCorp":
        # ChnSentiCorp æ˜¯ä¸€ä¸ªä¸­æ–‡é…’åº—è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†
        # çº¦ 9600 æ¡è®­ç»ƒæ•°æ®ï¼Œ1200 æ¡éªŒè¯/æµ‹è¯•æ•°æ®
        
        if local_path and os.path.exists(local_path):
            # ä»æœ¬åœ° HuggingFace ç¼“å­˜åŠ è½½
            print(f"ğŸ“‚ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†: {local_path}")
            # HuggingFace ç¼“å­˜çš„ Parquet æ ¼å¼
            data_dir = os.path.join(local_path, "data")
            if os.path.exists(data_dir):
                dataset = load_dataset(
                    "parquet",
                    data_dir=data_dir,
                )
            else:
                # å°è¯•ç›´æ¥åŠ è½½
                dataset = load_dataset(
                    local_path,
                )
        else:
            # ä» HuggingFace Hub åœ¨çº¿ä¸‹è½½
            dataset = load_dataset(
                "lansinuote/ChnSentiCorp",  # ä½¿ç”¨å·²è½¬æ¢ä¸º Parquet æ ¼å¼çš„ç‰ˆæœ¬
                cache_dir=cache_dir,
            )
        
    elif dataset_name == "IMDB_Chinese":
        # å¦‚æœä½¿ç”¨ IMDB ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬
        # éœ€è¦è‡ªè¡Œå‡†å¤‡æˆ–ä»å…¶ä»–æ¥æºè·å–
        dataset = load_dataset(
            "imdb",  # åŸå§‹ IMDBï¼Œéœ€è¦ç¿»è¯‘å¤„ç†
            cache_dir=cache_dir,
        )
        # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦è¿›è¡Œä¸­æ–‡ç¿»è¯‘å¤„ç†
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    return dataset


def load_local_dataset(
    data_dir: str,
    train_file: str = "train.json",
    eval_file: str = "eval.json",
    test_file: str = "test.json",
) -> DatasetDict:
    """
    ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        train_file: è®­ç»ƒé›†æ–‡ä»¶å
        eval_file: éªŒè¯é›†æ–‡ä»¶å
        test_file: æµ‹è¯•é›†æ–‡ä»¶å
    
    Returns:
        DatasetDict: æ•°æ®é›†å­—å…¸
    
    æ–‡ä»¶æ ¼å¼ï¼ˆJSON Linesï¼‰ï¼š
        {"text": "è¿™ä¸ªé…’åº—å¾ˆå¥½", "label": 1}
        {"text": "æœåŠ¡å¤ªå·®äº†", "label": 0}
    """
    
    data_files = {}
    
    train_path = os.path.join(data_dir, train_file)
    if os.path.exists(train_path):
        data_files["train"] = train_path
        
    eval_path = os.path.join(data_dir, eval_file)
    if os.path.exists(eval_path):
        data_files["validation"] = eval_path
        
    test_path = os.path.join(data_dir, test_file)
    if os.path.exists(test_path):
        data_files["test"] = test_path
    
    if not data_files:
        raise ValueError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
    
    dataset = load_dataset("json", data_files=data_files)
    
    return dataset


def get_data_collator(tokenizer: PreTrainedTokenizer) -> DataCollatorWithPadding:
    """
    è·å–æ•°æ®æ•´ç†å™¨
    
    ç”¨äºå°†ä¸åŒé•¿åº¦çš„æ ·æœ¬å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œå½¢æˆæ‰¹æ¬¡
    
    Args:
        tokenizer: åˆ†è¯å™¨
    
    Returns:
        DataCollatorWithPadding: æ•°æ®æ•´ç†å™¨
    """
    return DataCollatorWithPadding(
        tokenizer=tokenizer,
        padding=True,
        max_length=None,  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡æœ€å¤§é•¿åº¦
        return_tensors="pt",
    )


def inspect_dataset(dataset: DatasetDict) -> Dict[str, Any]:
    """
    æ£€æŸ¥æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯
    
    Args:
        dataset: æ•°æ®é›†
    
    Returns:
        åŒ…å«æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    info = {
        "splits": list(dataset.keys()),
        "features": str(dataset["train"].features) if "train" in dataset else None,
    }
    
    for split in dataset.keys():
        info[f"{split}_size"] = len(dataset[split])
        
        # æ ‡ç­¾åˆ†å¸ƒ
        if "label" in dataset[split].features:
            labels = dataset[split]["label"]
            info[f"{split}_label_distribution"] = {
                "negative (0)": labels.count(0),
                "positive (1)": labels.count(1),
            }
    
    return info


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("åŠ è½½ ChnSentiCorp æ•°æ®é›†...")
    dataset = load_sentiment_dataset("ChnSentiCorp")
    
    info = inspect_dataset(dataset)
    print("\næ•°æ®é›†ä¿¡æ¯ï¼š")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    print("\næ ·æœ¬ç¤ºä¾‹ï¼š")
    for i in range(3):
        sample = dataset["train"][i]
        print(f"  æ–‡æœ¬: {sample['text'][:50]}...")
        print(f"  æ ‡ç­¾: {'æ­£é¢' if sample['label'] == 1 else 'è´Ÿé¢'}")
        print()
